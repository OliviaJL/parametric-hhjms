\section{Introduction}
\label{sec:intro}



In longitudinal studies it is common that there are more than one response of interest. The longitudinal responses can have multiple types: longitudinal or survival data, continuous or discrete; they usually relate to each other. It requires, therefore, a modeling framework to measure the mixed types of responses and their relationships jointly. Computation of such models can be complicated as it usually contains an integration of high-dimensional parameters, which is intractable to solve.
Moreover, the problems of incomplete data, missing values, outliers, measurement errors and censoring, require special treatments in the models, and makes it more complex to solve the joint models.



Yu et al. \cite{yu2018joint} proposed a joint model to measure longitudinal processes of mixed types, a survival process, and their relationship. There is a truncated, continuous longitudinal response, a binary longitudinal process, and a survival process, all of which are correlated to each other. The responses can be measured in separate models, and then are linked to each other by shared random effects. The complex joint likelihood is solved by an approximate method, called  h-likelihood \cite{molas2013joint,ha2003joint,lee2018generalized}. Instead  of integrating the random effects out, it treats the random effects as parameters in the log h-likelihood function, and solves the sets of parameters sequentially in iteration. The h-likelihood method has a high computational efficiency, and converges to the optimal in reasonable iterations.

The semiparamteric joint modeling framework by Yu et al. \cite{yu2018joint} is further extended to parametric frameworks by replacing the semi-parametric model, Cox proportional hazards model, by parametric medels, e.g., Weibull regression models and other accelerated failure time models.

\subsection{Joint Models, Statistical Inference and Common Problems}

Model specification is important as the types of responses are complex and related to each other. Statistical inference based on the joint model can be complicated as the joint likelihood inference has an intractable integration of high-dimensional parameters. It is common that the longitudinal and survival data are incomplete. They may involve missing values, outliers, measurement errors, and can be censored, which require special treatments in the models.


\subsubsection*{\textit{Model Specification and Literature Review}}

In the cases that there are multiple longitudinal and survival processes of interest, researchers, first of all, need to identify the primary and secondary processes. The correct specification of models for primary processes is critical, and the secondary models can be simplified by setting the parameters as nuisance parameters. High-dimensional parameters can easily cause the problem of model non-identifiability in statistical inference, in which different sets of parameters can yield same likelihood function. A more parsimonious model, therefore, can potentially avoid this issue and lead to more efficient statistical inference. 
If each of the processes is measured in separate models and linked by shared parameters, the association parameters may be reduced to simplify the model.

For two or more longitudinal processes, they may be treated as responses in a single multivariate model or in separate models, or responses and time-dependent covariates in a single model. Models for longitudinal responses must address both the relationship between the response and covariates and the correlation in the repeated measurements. They need to measure two sources of variations: within-individual and between individual variations. There are three common models to measure longitudinal data: mixed effects models, marginal GEE models, and a transitional modelling approach. An advantage of mixed effects models is they allow individual-specific inference, and a restriction is that they have distributional assumptions which may not hold in some cases. Random effects in the mixed effects models represent the influence of each individual on the repeated observations that is not captured by the observed covariates. They accommodate the heterogeneity in the data, which may arise from subject or clustering effects or from spatial correlation. The magnitude of the random effects measures the variability across individuals or measures the between-individual variations. Semi-parametric or nonparametric mixed effects models relax the distributional assumptions.


Survival regression measures the dependence of event times on covariates. There are two main kinds of survival models: proportional hazards (PH) models and accelerated failure time (AFT) models. PH models have a proportional hazards assumption on the ratio of the hazard function of survival data and the baseline hazard. A commonly used PH model is Cox PH model, which is semi-parametric and do not have distributional assumption. Another common model is Weibull regression model, which can be represented either in a PH or a AFT form. Weibull regression models assume the survival data to follow a Weibull distribution, and the baseline hazard to follow a baseline Weibull distribution.AFT models are more interpretable than PH models as AFT models can be interpreted as the speed of disease progression. They are parametric models, and have distributional assumptions on survival data, but they relax the proportional hazards assumption. Though the distributional assumptions in parametric survival models can be restricted sometimes, they can lead to more efficient estimates if distributional assumptions hold as there are less parameters and much smaller corresponding asymptotic variance in parametric models.


If the longitudinal and survival data are associated, they need to be modeled jointly to avoid biased estimates. They can be linked by shared parameters or shared variables. Linked mixed effects models and frailty models, for example, can lead to shared random effects models. If the models are governed by same underlying processes, shared variables models are suitable to use. Lawrence et al in 2015 \cite{lawrence2015joint} discusses the existing joint models comprehensively. For truncated longitudinal processes, Bernhardt et al \cite{bernhardt2014flexible} proposed a joint model which treats the truncated data as a covariates in AFT survival models. Other related studies including \cite{krol2016joint,fu2017joint,barrett2015joint,elashoff2016joint} either have unverifiable assumption on truncated longitudinal variable, or are tedious in computation.


\subsubsection*{\textit{Joint Inference using Exact and Approximate Methods}}

A simple way to do statistical inference of joint model is a two-step method, which estimates the shared variables or parameters in one model, and then estimates the parameters in the other model separately using the estimated shared variables or parameters. The method is easy to implement as software is  available. It, however, can lead to biased estimates, especially when the longitudinal data and survival data are strongly correlated. It underestimates the uncertainty as the uncertainty of estimates in the first step is not incorporated in the second step. A third step, therefore, is usually needed to correct the bias and incorporate the estimation uncertainty in the first step, but it can be difficult for some complex problems. A more efficient joint inference method, thus, will be preferred.


A key difference between linear mixed effects (LME) models and other models including random effects is that the random effects are linear in the LME models, but nonlinear in the others. This leads to major computational challenges in likelihood estimation for models nonlinear in the random effects, since the likelihoods involve an intractable intergration. Thus, the joint models which are nonlinear with respect to the random effects are difficult to solve as closed-form or analytic expressions of parameter estimates are not available. Joint likelihood inference provides valid and reliable inference, and the resulting maximum likelihood estimates (MLEs) are asymptotically efficient and asymptotically normal under the usual regularity conditions. MLEs have nice asymptotic properties that they are consistent, asymptotically normal and asymptotically efficient under some conditions. It requires the model or parameter identifiability, but there are many unknown parameters and two sets of parameters are likely to yield same likelihood. 
Joint likelihood for longitudinal and survival models can be highly complicated as the integration with respect to high-dimensional, unobservable random effects can be intractable. The likelihood inference may be sensitive to departures from the assumed distributions and sensitive to outliers. An alternative model, GEE, have consistent estimates as long as the mean structure is correctly specified, even the covariance structure is misspecified.

There are three main types of methods to estimate the joint likelihood:  ``exact'' methods, EM algorithm and approximate methods. The ``exact'' methods, e.g., the Gauss-Hermite quadrature method or Monte Carlo methods, use submission to measure the integration with arbitrary accuracy. Its computation is intensive and even infeasible in high-dimensional cases, so it works well in relatively low-dimensional cases when random effects are normal. The Monte Carlo EM (ECM) algorithm can be used on any dimensions of parameters and any distributions. It treats the random effects as the additional ``missing data'', so that the random effects can be estimated as parameters and do not need to be integrated out. A downside is the ECM algorithm converges very slowly and it is easy to fail to converge. The approximate methods, e.g., Laplace or Taylor approximations, avoid solving the integration and can be much more computationally efficient. Good starting values are important due to the non-identifiability of the joint likelihood. The estimates can easily converge to a local optimization and fail to converge to the global optimization. 


\subsubsection*{\textit{Incomplete Data Problems in Joint Modeling}}


Given the unique characteristics of longitudinal or survival study, there are several common incomplete data problems \cite{wu2009mixed}, such as missing data, measurement errors, outliers and censoring, which may lead to biased data analysis results. Variables are measured over a period of time, and thus some subjects may fail to provide information at some time points in the middle of the study, which leads to a non-monotone missing pattern, or drop out the study before the last measurement, which results in a monotone missing pattern. Censoring exists when very large or small values are not measurable in practice. Outliers may exist and be influential to the results of some statistical methods, and thus need to be identified to reduce the bias in the analysis. There are two types of outliers in a longitudinal study: outliers of repeated measurements within individual subjects, and outliers in a variable at fixed time points. Survival data are often censored as the event of interest may not be observed for some subjects throughout the study period, skewed as they are usually not symmetric, and often have unequal follow-up times.


In separate longitudinal models, missing data can be easily identified, and there are multiple methods to eliminate the effect of missing values, e.g., complete removal and the last value carried forward (LVCF) method.
They are proper to use when the reason for missing is ignorable, i.e., data are missing at random (MAR) or completely at random (MCAR). However, they can lead to biased analysis results if the reason is nonignorable, i.e. data are missing not at random (MNAR). In this case, robust methods, such as multiple imputation (MI) method and expectation-maximization (EM) algorithm, are more appropriate to use as they can measure more uncertainty of missing values and lead to less biased results. A practical method, called a sensitivity analysis, can be used to compare the results of simple and robust methods for missing data in modelling, and select the preferred method to address the missing data. Outliers may be identified through plots sometimes, but it is inefficient in many longitudinal studies, e.g., when the number of subjects is very large. In this case, a heavy tail distribution, e.g., t-distribution, can be used to accommodate outliers. 
However, it is difficult to identify measurement errors of the observed data and censored data. Values on the boundary of the input space may be censored data, but may also be true observations. The impact of them may be eliminated by a sensitivity analysis comparing the results of a standard model and a robust models.


Missing data and measurement errors can occur in longitudinal or survival data, and in responses or covariates.
An additional binary, longitudinal process can be included if there are nonignorably incomplete data in responses or time-independent covariates. Whereas if incomplete data occur in time-dependent covariates, we need additional processes to model the incomplete mechanisms of covariates. Both of the methods make the joint model more complicated. This can lead to two major problems. The computation can be more complicated. Therefore, approximate methods which are more computationally efficient are highly valuable for joint models. Another major issue is the parameter identifiability that different sets of parameters can lead to the same joint likelihood. In summary, the joint model needs to be as simple as possible to avoid the problems of non-identifiability in computation, and to improve the precision of the estimates of the important parameters.


\subsection{The Joint Model for Longitudinal Data of Mixed Types and Survival Data}


Yu et al. in 2018 \cite{yu2018joint} proposed a joint model for multiple longitudinal and survival processes, called HHJMs. HHJMs measure multiple longitudinal processes including a truncated variable, as well as a survival process which solves the high-dimensional integration approximately with high computational efficiency. It is motivated by the HIV vaccine data where the immunization biomarkers are truncated to be larger than a specific value. The proposed model measures the truncated longitudinal variable by a couple of mixed effects models, one of which measuring the longitudinal trend and the other one measuring the truncation indicator. It is similar to the treatment on nonignorably missing data that we use a generalized linear mixed effects model to measure the binary indicator. This method is superior than previous methods as it avoids unverifiable assumption on truncated values.


The joint model is solved by an approximate method, called h-likelihood.
As is discussed in previous subsections, the computation and identifiability of the joint model are two major problems. HHJMs use an approximate method, called h-likelihood, to compute the joint likelihood which is much more computationally efficient than the exact methods, e.g., Gauss-Hermite method, or EM algorithm. The h-likelihood method treats the random effects as parameters. During the iteration, fixed effects, random effects and dispersion parameters are updated sequentially given the updated estimates from last iteration. The starting values of the parameters are critical, again, as the model can be non-identifiable, so that different sets of estimates can lead to same joint likelihood; furthermore, it is possible the parameters converge to sub-optimal solutions, or even stuck at dents that far from the optimal solutions and fail to converge after a large number of iterations.


HHJMs assume the missing data are missing at random, and do not include a missing data indicator for the jointly modeling. 
A reason is that the HIV vaccine data are collected from limited subjects, so that the model should not be too complicated to overfit the data. More importantly, the joint model should be parsimonious for the computational efficiency, and the missing data mechanism can include several parameters into the model and load more burden on the computation.



\subsection{The Proposed Parametric Joint Model}

HHJMs measure the survival data by a Cox proportional hazards model. It is a semi-parametric model that does not have distributional assumptions on survival data, but has a proportional hazards assumption that the hazards of survival data over the unspecified baseline hazards have a constant proportion which equals the linear predictor of the survival model. An advantage of the Cox model is that it does not have a restriction of distributions on data, but it can be less efficient if the distributional assumptions hold compared to parametric survival models.

It worth comparing the Cox model to parametric models, e.g., Weibull regression models and accelerated failure time (AFT) models. The Weibull regression model assumes the survival data follows a Weibull distribution and the baseline hazards  function is specified as a density of baseline Weibull distribution. It has the proportional hazards assumption as well. Parametric accelerated failure time models have distributional assumptions on survival data, and relaxes the proportional hazards assumption which may not be valid sometimes. This report focuses on the Weibull regression model and a parametric AFT model, Log-logistic AFT model, compares the performance of the three joint models using different survival models on real data. Simulation results are provided on the joint models using Cox and using Weibull models as limited time and computational resource.


\subsection{Overview of the Report}

This report explored the performance of the joint model of longitudinal data of mixed types and survival data, HHJMs, comparing to their parametric extensions. Section \ref{sec:intro} introduces the existing techniques for jointly modeling the longitudinal and survival processes, indicating the major problems relating to model identifiability, computation, and incomplete data in existing studies, discussing briefly the advantages and drawbacks of HHJMs and indicating possible extensions of HHJMs that will be discussed in the following of the report. Section \ref{sec:method} defines the HHJMs and the proposed parametric framework. Section \ref{sec:applc} uses the HHJMs and their parametric extensions on a real dataset and compare their performance. Section \ref{sec:simul} compares the HHJMs and the extension to Weibull regression models in synthetic data. Section \ref{sec:discuss}, in the end, summarises the advantages and drawbacks of HHJMs and their parametric extensions, and discusses several further studies.

