\section{Introduction}
\label{sec:intro}

%big picture: 
In longitudinal studies it is common that there are more than one response of interest. The responses can have multiple types. They can be longitudinal or survival data, continuous or discrete, and they usually relate to each other. It requires, therefore, modeling frameworks to measure the mixed types of responses and their relationships simultaneously. Computation of such models can be complicated as it usually contains an integration of high-dimensional parameters, which is intractable to solve.
Moreover, there are problems of incomplete data: missing values, outliers, measurement errors and censoring, which require special treatments in the models, and makes it more complex to solve the joint models.


% motivation case & special data type -> need special treatment/model
The study by Yu et al. \cite{yu2018joint} is motivated by the experiment of HIV immunization in which two longitudinal processes and a survival process are of the main interest. There is a truncated, continuous longitudinal response, a binary longitudinal process, and a survival process, all of which are correlated to each other.
% the HIV case: %(refer to the paper for more details of the case)
%there are two longitudinal responses that are of the main interests: a truncated continuous response NAb and a binary response; and a survival response
%The responses are related to each other.
As there is no one response that is more important than others, and they all have different types, the responses can be modeled separately in different models, and then correlate to each other by shared parameters, e.g., random effects.
%Therefore, this case requires a joint model that can capture both the data of mixed types and the relationship among responses simultaneously.


\subsection{Joint Models, Statistical Inference and Common Problems}

Model specification is important as the types of responses are complex and correctly measure the data is important. Statistical inference based on the joint model can be complicated as the joint likelihood inference has a high-dimensional integration of parameters which is intractable. It is common that the longitudinal and survival data are incomplete. Longitudinal data involve missing values, outliers, measurement errors, and they can be censored, and survival data can be censored. the incomplete data problems require special treatment in the joint models.

% mixed types of responses & correlation among them
\subsubsection*{\textit{Model Specification and Literature Review}}

In the cases that there are multiple longitudinal and survival processes of interest, researchers, first of all, need to identify the primary and secondary processes. The correct specification of models for primary processes is critical, and the secondary models can be simplified by setting the parameters as nuisance parameters. High-dimensional parameters can easily cause the problem of model non-identifiability in statistical inference, in which difference sets of parameters can lead to same performance. A more parsimonious model, therefore, can potentially avoid this issue and lead to more efficient statistical inference. 
If both longitudinal and survival processes are of the main interest, the models for longitudinal and survival data, for example, can be linked by shared parameters. The association parameters may be reduced.
%they may share the same random effects since these random effects characterize the individual-specific longitudinal processes.

%If the longitudinal data and survival data are both of interest in the case, they need to be modeled jointly to study their relationship and avoid possible bias. 
%
%measure responses separately in different models, and connect them by sharing parameters or variables.
% longitudinal
For two or more longitudinal processes, they may be treated as responses in a single multivariate model or in separate models, or responses and time-dependent covariates in a single model. 
%In the former case we are interested in their association over time, and in the latter case we are 
Models for longitudinal responses must address both the relationship between the response and covariates and the correlation in the repeated measurements. They must measure two sources of variations: within-individual and between individual variations. There are three common models to measure longitudinal data: mixed effects models, marginal GEE models, and a transitional modelling approach. An advantage of mixed effects models is they allow individual-specific inference. A restriction is mixed effects models have distributional assumptions which may not hold in some cases. Random effects in the mixed effects models represent the influence of each individual on the repeated observations that is not captured by the observed covariates. They accommodate the heterogeneity in the data, which may arise from subject or clustering effects or from spatial correlation. The magnitude of the random effects measures the variability across individuals or measures the between-individual variations.
%Mixed effects models are called subject-specific models, which is contrasted with the population averaged models or marginal models.
Semi-parametric or nonparametric mixed effects models relax the distributional assumptions.

% survival
Survival regression measures the dependence of event times on covariates. There are two main kinds of survival models: proportional hazards (PH) models and accelerated failure time (AFT) models. PH models have an assumption of constant hazard ratio over time which is a proportion of the hazard function of survival data and the baseline hazard. A commonly used PH model is Cox PH model, which is semi-parametric and do not have distributional assumption. Another common model is Weibull regression model, which can be treated either as a PH or a AFT model. Weibull regression models are assumed the survival data follow a Weibull distribution. Whereas, AFT models are parametric models. AFT models are more interpretable than PH models as AFT models can be interpreted as the speed of disease progression. It relaxes the proportional hazards assumption. 
Parametric survival models are more efficient if distributional assumptions hold, whereas the distributional assumptions can be restricted sometimes. %AFT models, compared to PH models, offer better interpretations on survival data.


If the longitudinal and survival data are associated, they need to modeled jointly to avoid biased estimates. They can be linked by shared parameters or shared variables. Linked mixed effects models and frailty models, for example, can lead to shared random effects models. If the models are governed by same underlying process, shared variables models are suitable.
% correlation: dependent parameters or dependent covariates
% random effects & correlation among responses via random effects
Lawrence et al in 2015 discusses the existing joint models comprehensively \cite{lawrence2015joint}. For truncated longitudinal processes, Bernhardt et al \cite{bernhardt2014flexible} proposed a joint model which treats the truncated data as a covariates in AFT survival models \cite{bernhardt2014flexible}.

% joint model for the different types of related responses & literature review
% literature review of the models that solve similar problems, but not quite suitable for the case (advantages and drawbacks)




\subsubsection*{\textit{Joint Inference using Exact and Approximate Methods}}

%joint inference is required.

A simple way to do statistical inference of joint model is a two-step method, which estimates the shared variables or parameters in one model, and then estimates the parameters in the other model separately using the estimated shared variables or parameters. The method is easy to implement as software is  available. It, however, can lead to biased estimates, especially when the longitudinal data and survival data are strongly correlated. It underestimates the uncertainty as the uncertainty of estimates in the first step is not incorporated in the second step. A third step, therefore, is usually needed to correct the bias and incorporate the estimation uncertainty in the first step, but it can be difficult for some complex problems.  
%it is also called regression calibration method in measurement error literature.
%
A more efficient joint inference method is preferred.


A key difference between LME and GLMM/NLME/Frailty is that the random effects are linear in the LME models, but nonlinear in the others. This leads to major computational challenges in likelihood estimation for models nonlinear in the random effects, since the likelihoods involve..
the difficulties: GLME models are used; therefore, cannot find a closed-form or analytic expressions of parameter estimates
%
Joint likelihood inference provides valid and reliable inference, and the resulting maximum likelihood estimates (MLEs) are asymptotically efficient and asymptotically normal under the usual regularity conditions. 
%advantages
MLEs have nice asymptotic properties that they are consistent, asymptotically normal and asymptotically efficient under some conditions
the likelihood inference of mixed effects models are conceptually straight-forward and MLEs have very attractive asymptotic properties
%
It requires the model or parameter identifiability, but there are many unknown parameters and two sets of parameters are likely to yield same likelihood. 
Joint likelihood for longitudinal and survival models can be highly complicated as the integration with respect to high-dimensional, unobservable random effects can be intractable. %, censoring and the semiparametric or nonlinear structures of the models
%it can be intractable except for LME models.
% Computation of likelihood function of such models is challenging as there are high-dimensional parameters and the integration of high-dimensional random effects is intractable.
%
%a drawback is the distributional assumption
%drawback: the likelihood method requires strong assumptions such as linearity
The likelihood inference may be sensitive to departures from the assumed distributions and sensitive to outliers.
%GEEs are less efficient than the likelihood estimates if the distributional assumptions hold
%quasi-likelihood method are more robust, and closely related to GEEs in which only the first two moments are needed to specify
%
%a drawback is the distributional assumption
%drawback: the likelihood method requires strong assumptions such as linearity
The likelihood inference may be sensitive to departures from the assumed distributions and sensitive to outliers.
%GEEs are less efficient than the likelihood estimates if the distributional assumptions hold
%quasi-likelihood method are more robust, and closely related to GEEs in which only the first two moments are needed to specify
GEE estimates are consistent as long as the mean structure is correctly specified, even the covariance structure is misspecified; asymptotically normal; not fully efficient


% measurement: exact and approximate solutions
% EM algorithm, Laplacian - h-likelihood
There are three main types of methods to estimate the joint likelihood:  ``exact'' methods, EM algorithm and approximate methods.
The ``exact'' methods, e.g., the Gauss-Hermite quadrature method or Monte Carlo methods, use submission to measure the integration with arbitrary accuracy. Its computation is intensive and even infeasible in high-dimensional cases, so it works well in relatively low-dimensional cases when random effects are normal.
The Monte Carlo EM (ECM) algorithm can be used on any dimensions of parameters and any distributions. It treats the random effects as the additional ``missing data'', so that the random effects can be estimated as parameters and do not need to be integrated out. A downside is the ECM algorithm converges very slowly and it is easy to fail to converge.
%It uses an E-step and an M-step to ..until convergence, but it still suffers the problem of slow convergence and intensive computation.
The approximate methods, e.g., Laplace or Taylor approximations, avoid solving the integration and can be much more computationally efficient.
%There are Monte Carlo EM algorithm for exact likelihood inference that can be computationally intensive, and approximate methods that are much more efficient in computation.
%the approximate method consists of a first-order Laplace approximation
%
Given the non-identifiability of the joint likelihood, good starting values are important as it can easily converge to a local optimization and fail to converge to the global optimization. 
%Other problems: model selection, covariate selection, parameter selection, etc.
%model selection: use parsimonious models to avoid potential problem of collinearity, and improve the precision of the main parameter estimates  %(why improve precision?)


% incomplete data problems
\subsubsection*{\textit{Incomplete Data Problems in Joint Modeling}}

% in separate models
Given the unique characteristics of longitudinal or survival study, there are several common incomplete data problems \cite{wu2009mixed}, such as missing data, measurement errors, outliers and censoring, which may potentially lead to biased data analysis results. Variables are measured over a period of time, and thus some subjects may fail to provide information at some time points in the middle of the study, which leads to a non-monotone missing pattern, or drop out the study before the last measurement, which results in a monotone missing pattern. Censoring exists when very large or small values are not measurable in practice. Outliers may exist and be influential to the results of some statistical methods, and thus need to be identified to reduce the bias in the analysis. There are two types of outliers in a longitudinal study: outliers of repeated measurements within individual subjects, and outliers in a variable at fixed time points. 
%Other data problems may exist in practice, but the four incomplete data problems are the most common ones, and thus are the main focus in the report. 
%
Survival data are often censored as the event of interest may not be observed for some subjects throughout the study period, skewed as they are usually not symmetric, and often have unequal follow-up times.



% missing data
In separate longitudinal models, missing data can be easily identified, and there are multiple methods to eliminate the effect of missing values, e.g., complete removal and the last value carried forward (LVCF) method.
They are proper to use when the reason for missing is ignorable, i.e., data are missing at random (MAR) or completely at random (MCAR). However, they can lead to biased analysis results if the reason is nonignorable, i.e. data are missing not at random (MNAR). In this case, robust methods, such as multiple imputation (MI) method and expectation-maximization (EM) algorithm, are more appropriate to use as they can measure more uncertainty of missing values and lead to less biased results. %The analysis of missing data modeling is related to several aspects: the variable is the response or a covariate; it is time-dependent or time-independent of it is a covariate; if is categorical or quantitative; the reason for missing is ignorable or not. The selection of missing data methods also depends on the statistical modeling approaches. 
A practical method, called a sensitivity analysis, can be used to compare the results of simple and robust methods for missing data in modelling, and select the preferred method to address the missing data. 
% outliers
Outliers may be identified through plots sometimes, but it is inefficient in many longitudinal studies, e.g., when the number of subjects is very large. In this case, a heavy tail distribution, e.g., t-distribution, can be used to accommodate outliers. 
However, it is difficult to identify measurement errors of the observed data and censored data. Values on the boundary of the input space may be censored data, but may also be true observations. The impact of them may be eliminated by a sensitivity analysis comparing the results of a standard model and a robust models.


% in joint modeling:
Missing data and measurement errors can occur in longitudinal or survival data, and in responses or covariates.
An additional binary, longitudinal process can be included if there are nonignorably incomplete data in responses or time-independent covariates. Whereas if incomplete data occur in time-dependent covariates, we need additional processes to model the incomplete mechanisms of covariates. Both of the methods make the joint model more complicated. This can lead to two major problems. The computation can be more complicated. Therefore, approximate methods which are more computationally efficient are highly valuable for joint models. Another major issue is the parameter identifiability that different sets of parameters can lead to the same joint likelihood. In summary, the joint model needs to be as simple as possible to avoid the problems of non-identifiability in computation, and improve the precision of the estimates of the important parameters.
%
% Data type and potential measurement problems, variable relationship -> the kind of model needed
% what information should be measured in the data: data types, relationships, etc.
%Mixed effects models are straightforward to incorporate missing data or measurement errors in likelihood inference for mixed effects models, even when the missing data mechanism is non-ignorable.




% the proposed semi-parametric modeling framework HHJMs
\subsection{The Joint Model for Longitudinal Data of Mixed Types and Survival Data}


% model specification: 
%how the HHJMs deal with the mixed types of responses (advantages and drawbacks)
Yu et al. in 2018 \cite{yu2018joint} proposed a joint model for multiple longitudinal and survival processes, called HHJMs. HHJMs measure multiple longitudinal process including a truncated variable, as well as a survival process, by h-likelihood, which solves the high-dimensional integration approximately with high computational efficiency. It is motivated by the HIV vaccine data where the immunization biomarkers are truncated to be larger than a specific value. The proposed model measures the truncated longitudinal variable by a couple of mixed effects models, one of which measuring the longitudinal trend and the other one measuring the truncation indicator. It is similar to the treatment on nonignorably missing data that we use a generalized linear mixed effects model to measure the binary indicator. This method is superior than previous methods as it avoids unverifiable assumption on truncated values.

% h-likelihood
As is discussed in previous subsections, the computation and identifiability of the joint model are two major problems. HHJMs use an approximate method, called h-likelihood, to compute the joint likelihood which is much more computationally efficient than the exact methods, e.g., Gauss-Hermite method, or EM algorithm. The h-likelihood method treats the random effects as parameters. During the iteration, fixed effects, random effects and dispersion parameters are updated sequentially given the updated estimates from last iteration. The starting values of the parameters are critical, again, as the model can be non-identifiable, so that different sets of estimates can lead to same joint likelihood; furthermore, it is possible the parameters converge to sub-optimal solutions, or even stuck at dents that far from the optimal solutions and fail to converge after a large number of iterations.
%Instead of integrating out the random effects as in linear mixed effects models, it measures the sum of random effects.


% other problems: incomplete data, common solutions and how the HHJMs deal with them
%As in the HIV immunization case there are two types of longitudinal data, they cannot be modeled in a single multivariate model, and should be measured in separate models.
HHJMs assume the missing data are missing at random, and do not include a missing data mechanism into the joint model. 
A reason is that the HIV vaccine data are collected from limited subjects, so that the model should not be too complicated to overfit the data. More importantly, the joint model should be parsimonious for the computational efficiency, and the missing data mechanism can include several parameters into the model and load more burden on the computation.





% parametric HHJMs
\subsection{The Proposed Parametric Joint Model}
% the extension to eliminate the limitation (parametric HHJMs): similarity and differences to HHJMs, advantages and limitations
% models and approximation methods

HHJMs measure the survival data by a Cox proportional hazards model. It is a semi-parametric model that do not have distributional assumption on survival data, but have a proportional hazards assumption that the hazards of survival data over the unspecified baseline hazards have a constant proportion which equals the linear predictor of the survival model. An advantage of the Cox model is it does not have a restriction of distributions on data, but it can be less efficient if the distributional assumptions hold compared to parametric survival models.

It worth comparing the Cox model to parametric models, e.g., Weibull regression models and accelerated failure time (AFT) models. The Weibull regression model assumes the survival data follows a Weibull distribution and the baseline hazards  function is specified as a density of baseline Weibull distribution. It has the proportional hazards assumption as well. Parametric accelerated failure time models have distributional assumptions on survival data, and relaxes the proportional hazards assumption which may not be valid sometimes. This report focuses on the Weibull regression model and a parametric AFT model, Log-logistic AFT model, compares the performance of the three joint models using different survival models on real data. Simulation results are provided on the joint models using Cox and using Weibull models as limited time and computational resource.


\subsection{Overview of the Report}

This report explored the performance of the joint model of longitudinal data of mixed types and survival data, HHJMs, comparing to their parametric extensions. Section \ref{sec:intro} introduces the existing techniques for jointly modeling the longitudinal and survival processes, indicating the major problems relating to model identifiability, computation, and incomplete data in existing studies, discussing briefly the advantages and drawbacks of HHJMs and pointing out the possible extensions of HHJMs that will be discussed in the following of the report. Section \ref{sec:method} defined the HHJMs and the proposed parametric framework. Section \ref{sec:applc} use the HHJMs and their parametric extensions on a real dataset and compare their performance. Section \ref{sec:simul} compares the HHJMs and the extension to Weibull regression models in synthetic data. Section \ref{sec:discuss}, in the end, summarises the advantages and drawbacks of HHJMs and their parametric extensions, and discusses several further studies.

%?why propose a new approach to estimate the standard errors of the  h-likelihood based parameter estimates by using an adaptive Gauss-Hermite method??



% Lang's book
%\cite{wu2009mixed}

%longitudinal studies:
%multiple measurements of a variable on the same individuals are correlated
%study changes of variables over time
%the correlation among data should be incorporated in the analysis in order to avoid potential bias and loss of efficiency.
%- allow change over time, and allow unbalanced data
%- info from different individuals in statistical inference

%missing data and dropouts are common,
%which may not be the case in clustered data or repeated measurement data


%FOUR main problems: - missing data and dropouts;- measurement errors;- censoring;- outliers
%may lead to severely biased or misleading results
%solve the problems to have a reliable result.

%longitudinal study: 
%- unbalanced numbers of measurements and measurement times across individuals
%- within-individual repeated measurements are correlated
%- substantial variation in both within-individual and between-individual measurement
%- incomplete data: FOUR main problems

%in a regression model, covariates are used to partially explain the systematic variation in the response, the remaining unexplained variation in the response is treated as random and is often assumed to follow a probability distribution

%unknown parameters be estimated by least squares method or maximum likelihood method

%model checking or model diagnostics should be performed to check the reasonability of the model and the assumptions
%informally based on residual plots and other graphical techniques

%variable transformation may be used to improve model fitting

%outliers and influential observations should be checked since they may greatly affect the resulting estimates and may lead to misleading inference

